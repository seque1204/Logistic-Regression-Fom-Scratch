# ðŸ§  Logistic Regression from Scratch

This project implements a **numerically stable logistic regression classifier** using only NumPy and Matplotlib â€” no deep learning libraries required.

It includes a full training loop, gradient computation, custom loss function, and evaluation on synthetic data generated by `sklearn.datasets.make_moons`.

---

## ðŸš€ Features

- Implements logistic regression with:
  - Custom sigmoid and log-sigmoid functions for **numerical stability**
  - **Binary cross-entropy loss** from scratch
  - Manual weight and bias updates via gradient descent
- Plots training data and loss progression
- Evaluates model accuracy on synthetic classification data

---

## ðŸ“Š Example Output

- **Training Accuracy**: ~85â€“90% on `make_moons` data  
- **Test Accuracy**: ~85-90%% depending on noise/random seed  
- **Visualizations**:
  - Class distribution in 2D space
  - Loss curve across iterations

---

## ðŸ§ª How It Works

- **Data**: Uses `sklearn.make_moons(n_samples, noise=0.1)` for non-linear classification challenge
- **Model**: `z = Xw + b`  
- **Loss**: Custom binary cross-entropy using stable `log(sigmoid)` and `log(1 - sigmoid)` functions
- **Optimization**: Gradient descent for weights `w` and bias `b`

---

## ðŸ“¦ Dependencies

- `numpy`
- `matplotlib`
- `scikit-learn`
- `pandas`

Install them with:

```bash
pip install numpy matplotlib scikit-learn pandas

